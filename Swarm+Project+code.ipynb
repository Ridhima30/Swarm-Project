{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "5mQ-hsgeMXSr",
    "outputId": "a23e64d0-573c-414b-aabb-8e298bacd014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\RIDHIMA\n",
      "[nltk_data]     BANSAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, unicode_literals\n",
    "\n",
    "\n",
    "from sys import version_info\n",
    "\n",
    "PY3 = version_info[0] == 3\n",
    "\n",
    "# Use classical Snowball stemmer for english\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "#Snowball is a small string processing language designed for creating stemming algorithms for use in Information Retrieval.\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# class WordTokenizer(object):\n",
    "# \t\"\"\"docstring for WordTokenizer\"\"\"\n",
    "# \tdef __init__(self):\n",
    "# \t\tpass\n",
    "# \tdef tokenize(self, arg):\n",
    "# \t\treturn nltk.word_tokenize(arg)\n",
    "# tokenizer =  WordTokenizer()\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopset = frozenset(stopwords.words('english'))\n",
    "\n",
    "# Convert an object to its unicode representation (if possible)\n",
    "def to_unicode(object):\n",
    "\tif isinstance(object, str):\n",
    "\t\treturn object\n",
    "\telif isinstance(object, bytes):\n",
    "\t\treturn object.decode(\"utf8\")\n",
    "\telse:\n",
    "\t\tprint (object)\n",
    "\t\tif PY3:\n",
    "\t\t\tif hasattr(instance, \"__str__\"):\n",
    "\t\t\t\treturn unicode(instance)\n",
    "\t\t\telif hasattr(instance, \"__bytes__\"):\n",
    "\t\t\t\treturn bytes(instance).decode(\"utf8\")\n",
    "\t\telse:\n",
    "\t\t\tif hasattr(instance, \"__unicode__\"):\n",
    "\t\t\t\treturn unicode(instance)\n",
    "\t\t\telif hasattr(instance, \"__str__\"):\n",
    "\t\t\t\treturn bytes(instance).decode(\"utf8\")\n",
    "\n",
    "# normalize and stem the word\n",
    "def stem_word(word):\n",
    "\treturn stemmer.stem(normalize_word(word))\n",
    "\n",
    "# convert to unicode and convert to lower case\n",
    "def normalize_word(word):\n",
    "\treturn to_unicode(word).lower()\n",
    "\n",
    "# convert the sentence to a list of tokens\n",
    "def sentence_tokenizer(sentence):\n",
    "\treturn tokenizer.tokenize(sentence)\n",
    "\n",
    "def get_len(element):\n",
    "\treturn len(tokenizer.tokenize(element))\n",
    "\n",
    "def get_ngrams(sentence, N):\n",
    "\ttokens = tokenizer.tokenize(sentence.lower())\n",
    "\tclean = [stemmer.stem(token) for token in tokens]\n",
    "\treturn [gram for gram in ngrams(clean, N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8FpPoYEOMXSv"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-256f3fe3a79d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mipynb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSteps\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGA_Steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#A processing interface for removing morphological affixes from words. This process is known as stemming.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipynb'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from ipynb.fs.full.Steps import GA_Steps\n",
    "\n",
    "#A processing interface for removing morphological affixes from words. This process is known as stemming.\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_all_content_words_lemmatized(sentences, N=1):\n",
    "\tall_words = []\n",
    "\tfor s in sentences:\n",
    "\t\tall_words.extend([wordnet_lemmatizer.lemmatize(r) for r in tokenizer.tokenize(s)])\n",
    "\tif N == 1:\n",
    "\t\tcontent_words = [w for w in all_words if w not in stopset]\n",
    "\tnormalized_content_words = map(normalize_word, content_words)\n",
    "\tif N > 1:\n",
    "\t\treturn [gram for gram in ngrams(normalized_content_words, N)]\n",
    "\treturn normalized_content_words\n",
    "\n",
    "def get_all_content_words_stemmed(sentences, N=1):\n",
    "\tdef is_ngram_content(g):\n",
    "\t\tfor a in g:\n",
    "\t\t\tif not(a in stopset):\n",
    "\t\t\t\treturn True\n",
    "\t\treturn False\n",
    "\n",
    "\tall_words = []\n",
    "\tfor s in sentences:\n",
    "\t\tall_words.extend([stemmer.stem(r) for r in tokenizer.tokenize(s)])\n",
    "\n",
    "\tif N == 1:\n",
    "\t\tcontent_words = [w for w in all_words if w not in stopset]\n",
    "\telse:\n",
    "\t\tcontent_words = all_words\n",
    "\n",
    "\tnormalized_content_words = map(normalize_word, content_words)\n",
    "\tif N > 1:\n",
    "\t\treturn [gram for gram in ngrams(normalized_content_words, N) if is_ngram_content(gram)]\n",
    "\treturn normalized_content_words\n",
    "\n",
    "def get_all_content_words(sentences, N=1):\n",
    "\tall_words = []\n",
    "\tfor s in sentences:\n",
    "\t\tall_words.extend(tokenizer.tokenize(s))\n",
    "\tcontent_words = [w for w in all_words if w not in stopset]\n",
    "\tnormalized_content_words = map(normalize_word, content_words)\n",
    "\tif N > 1:\n",
    "\t\treturn [gram for gram in ngrams(normalized_content_words, N)]\n",
    "\treturn normalized_content_words\n",
    "\n",
    "def get_content_words_in_sentence(sentence):\n",
    "\twords = tokenizer.tokenize(sentence)\n",
    "\treturn [w for w in words if w not in stopset]\n",
    "\n",
    "def compute_tf_doc(docs, N=1):\n",
    "\tsentences = []\n",
    "\tfor title, doc in docs:\n",
    "\t\tsentences.append(title)\n",
    "\t\tsentences.extend(doc)\n",
    "\n",
    "\tcontent_words = list(set(get_all_content_words_stemmed(sentences, N)))\n",
    "\tdocs_words = []\n",
    "\tfor title, doc in docs:\n",
    "\t\ts_tmp = [title]\n",
    "\t\ts_tmp.extend(doc)\n",
    "\t\tdocs_words.append(get_all_content_words_stemmed(s_tmp, N))\n",
    "\n",
    "\tword_freq = {}\n",
    "\tfor w in content_words:\n",
    "\t\tw_score = 0\n",
    "\t\tfor d in docs_words:\n",
    "\t\t\tif w in d:\n",
    "\t\t\t\tw_score += 1\n",
    "\t\tif w_score != 0:\n",
    "\t\t\tword_freq[w] = w_score\n",
    "\n",
    "\tcontent_word_tf = dict((w, f / float(len(word_freq.keys()))) for w, f in word_freq.items())\n",
    "\treturn content_word_tf\n",
    "\n",
    "def compute_word_freq(words):\n",
    "\tword_freq = {}\n",
    "\tfor w in words:\n",
    "\t\tword_freq[w] = word_freq.get(w, 0) + 1\n",
    "\treturn word_freq\n",
    "\n",
    "def compute_tf(sentences, N=1):\n",
    "\tcontent_words = get_all_content_words_stemmed(sentences, N)\n",
    "\tcontent_words_count = len(list(content_words))\n",
    "\tcontent_words_freq = compute_word_freq(content_words)\n",
    "\n",
    "\tcontent_word_tf = dict((w, f / float(content_words_count)) for w, f in content_words_freq.items())\n",
    "\treturn content_word_tf\n",
    "\n",
    "def compute_average_freq(l_freq_1, l_freq_2):\n",
    "\taverage_freq = {}\n",
    "\n",
    "\tkeys = set(l_freq_1.keys()) | set(l_freq_2.keys())\n",
    "\n",
    "\tfor k in keys:\n",
    "\t\ts_1 = l_freq_1.get(k, 0)\n",
    "\t\ts_2 = l_freq_2.get(k, 0)\n",
    "\n",
    "\t\taverage_freq[k] = (s_1 + s_2) / 2.\n",
    "\n",
    "\treturn average_freq\n",
    "\n",
    "def kl_divergence(summary_freq, doc_freq):\n",
    "\tsum_val = 0\n",
    "\tfor w, f in summary_freq.items():\n",
    "\t\tif w in doc_freq:\n",
    "\t\t\tsum_val += f * math.log(f / float(doc_freq[w]))\n",
    "\t\n",
    "\treturn sum_val\n",
    "\n",
    "def js_divergence(sys_summary, doc_freq):\n",
    "\tsummary_freq = compute_tf(sys_summary)\n",
    "\taverage_freq = compute_average_freq(summary_freq, doc_freq)\n",
    "\n",
    "\tjsd = kl_divergence(summary_freq, average_freq) + kl_divergence(doc_freq, average_freq)\n",
    "\treturn jsd / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hzy67J7GMXSx"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_len(element):\n",
    "\treturn len(tokenizer.tokenize(element))\n",
    "\t\n",
    "def greedy_optimizer(sorted_elements, K):\n",
    "\tavailable_space = K\n",
    "\tsummary = []\n",
    "\n",
    "\twhile True:\n",
    "\t\tfor element in sorted_elements:\n",
    "\t\t\tlen_element = get_len(element[0])\n",
    "\t\t\tif available_space - len_element >= 0:\n",
    "\t\t\t\tidx = sorted_elements.index(element)\n",
    "\t\t\t\tdel sorted_elements[idx]\n",
    "\t\t\t\tavailable_space -= len_element\n",
    "\t\t\t\tsummary.append(element[0])\n",
    "\t\telse:\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kbOACe-MXSz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "#from greedy import greedy_optimizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "class GeneticOptimizer(object):\n",
    "\tdef __init__(self, fitness_fun, docs, docs_representation, max_length, population_size, survival_rate, mutation_rate, reproduction_rate, maximization=False, sentences_rep=None):\n",
    "\t\tnp.random.seed(123)\n",
    "\n",
    "\t\tself._fitness_fun = fitness_fun\n",
    "\t\tself._population_size = population_size\n",
    "\t\tself._survival_rate = survival_rate\n",
    "\t\tself._mutation_rate = mutation_rate\n",
    "\t\tself._reproduction_rate = reproduction_rate\n",
    "\t\tself._maximization = maximization\n",
    "\n",
    "\t\tself._docs = docs\n",
    "\t\tself._docs_representation = docs_representation\n",
    "\t\tself._sentences_rep = sentences_rep\n",
    "\t\tself._max_length = max_length\n",
    "\t\t\n",
    "\t\tself._sentences = []\n",
    "\t\tself._sentence_tokens = []\n",
    "\t\tfor title, doc in docs:\n",
    "\t\t\tself._sentences.append(title)\n",
    "\t\t\tself._sentence_tokens.append(tokenizer.tokenize(title))\n",
    "\t\t\tself._sentences.extend(doc)\n",
    "\t\t\tfor s in doc:\n",
    "\t\t\t\tself._sentence_tokens.append(tokenizer.tokenize(s))\n",
    "\n",
    "\tdef _create_random_individual(self):\n",
    "\t\trandom_scores = np.random.rand(len(self._sentences))\n",
    "\t\tscored_sentences = zip(self._sentences, random_scores)\n",
    "\t\tsorted_sentences = sorted(scored_sentences, key=lambda tup: tup[1], reverse=True)\n",
    "\t\treturn greedy_optimizer(sorted_sentences, self._max_length)\n",
    "\n",
    "\tdef _generate_random_population(self, n):\n",
    "\t\tpopulation = []\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tpopulation.append(self._create_random_individual())\n",
    "\t\treturn population\n",
    "\n",
    "\tdef _score_population(self, population):\n",
    "\t\tscored_population = []\n",
    "\t\tfor individual in population:\n",
    "\t\t\t# score = self._fitness_fun(individual, self._docs)\n",
    "\t\t\tif self._sentences_rep != None:\n",
    "\t\t\t\tscore = self._fitness_fun(individual, self._docs_representation, self._sentences_rep)\n",
    "\t\t\telse:\n",
    "\t\t\t\tscore = self._fitness_fun(individual, self._docs_representation)\n",
    "\t\t\tscored_population.append((individual, score))\n",
    "\n",
    "\t\treturn scored_population\n",
    "\n",
    "\tdef _select_survivors(self, scored_population):\n",
    "\t\tsorted_population = sorted(scored_population, key=lambda tup: tup[1], reverse=self._maximization)\n",
    "\n",
    "\t\tpercentage_winner = 0.5\n",
    "\n",
    "\t\tto_keep = int(self._survival_rate * self._population_size)\n",
    "\t\tnumber_winners = int(percentage_winner * to_keep)\n",
    "\t\twinners = [tup[0] for tup in sorted_population[:number_winners]]\n",
    "\t\t\n",
    "\t\tlosers = sorted_population[number_winners:]\n",
    "\n",
    "\t\tnumber_losers = int((1 - percentage_winner) * to_keep) \n",
    "\n",
    "\t\tsurvivors = deepcopy(winners)\n",
    "\t\trandom_scores = np.random.rand(len(losers))\n",
    "\n",
    "\t\tsorted_losers = sorted(zip(losers, random_scores), key=lambda tup: tup[1])\n",
    "\t\tloser_survivors = [tup[0][0] for tup in sorted_losers[:number_losers]]\n",
    "\n",
    "\t\tsurvivors.extend(loser_survivors)\n",
    "\t\treturn survivors, winners\n",
    "\n",
    "\tdef _new_generation(self, scored_population):\n",
    "\t\tnew_generation, winners = self._select_survivors(scored_population)\n",
    "\t\tnew_generation = self._mutate(new_generation)\n",
    "\t\tnew_generation.extend(self._reproduction(winners, len(new_generation)))\n",
    "\t\tindividuals_to_create = self._population_size - len(new_generation)\n",
    "\t\tnew_generation.extend(self._generate_random_population(individuals_to_create))\n",
    "\t\t\n",
    "\t\treturn new_generation\n",
    "\n",
    "\tdef _len_individual(self, individual):\n",
    "\t\tlen_ = 0\n",
    "\t\tfor sentence in individual:\n",
    "\t\t\tlen_ += len(tokenizer.tokenize(sentence))\n",
    "\t\treturn len_\n",
    "\n",
    "\tdef _mutate(self, population, mutation_rate=\"auto\"):\n",
    "\t\tif mutation_rate == \"auto\":\n",
    "\t\t\tmutation_rate = self._mutation_rate\n",
    "\n",
    "\t\tnb_mutant = int(mutation_rate * len(population))\n",
    "\n",
    "\t\trandom_scores = np.random.rand(len(population))\n",
    "\t\tsorted_population = sorted(zip(population, random_scores), key=lambda tup: tup[1])\n",
    "\t\tmutants = [tup[0] for tup in sorted_population[:nb_mutant]]\n",
    "\n",
    "\t\tmutated = []\n",
    "\t\ti = 0\n",
    "\t\tfor mutant in mutants:\n",
    "\t\t\tto_mutate = deepcopy(mutant)\n",
    "\n",
    "\t\t\tsentence_to_remove = random.choice(to_mutate)\n",
    "\t\t\tidx = to_mutate.index(sentence_to_remove)\n",
    "\t\t\tdel to_mutate[idx]\n",
    "\n",
    "\t\t\tavailable_size = self._max_length - self._len_individual(to_mutate)\n",
    "\t\t\n",
    "\t\t\tavailable_sentences = [s[0] for s in zip(self._sentences, self._sentence_tokens) if len(s[1]) <= available_size]\n",
    "\t\t\tif available_sentences != []:\n",
    "\t\t\t\ti += 1\n",
    "\t\t\t\tsentence_to_add = random.choice(available_sentences)\n",
    "\t\t\t\tto_mutate.append(sentence_to_add)\n",
    "\t\t\t\t\n",
    "\t\t\t\tmutated.append(to_mutate)\n",
    "\t\t\n",
    "\t\tpopulation.extend(mutated)\n",
    "\t\treturn population\n",
    "\n",
    "\tdef _reproduction(self, population_winners, population_size, reproduction_rate=\"auto\"):\n",
    "\t\tif reproduction_rate == \"auto\":\n",
    "\t\t\treproduction_rate = self._reproduction_rate\n",
    "\n",
    "\t\tparents = []\n",
    "\t\tnumber_families = int(reproduction_rate * population_size)\n",
    "\t\n",
    "\t\tfor i in range(number_families):\n",
    "\t\t\tparents.append(random.sample(population_winners, 2))\n",
    "\n",
    "\t\tchildren = []\n",
    "\t\tfor father, mother in parents:\n",
    "\t\t\tgenetic_pool = [s for s in self._sentences if s in father]\n",
    "\t\t\tgenetic_pool.extend([s for s in self._sentences if s in mother])\n",
    "\t\t\trandom_scores = np.random.rand(len(genetic_pool))\n",
    "\t\t\tscored_sentences = zip(self._sentences, random_scores)\n",
    "\t\t\tsorted_sentences = sorted(scored_sentences, key=lambda tup: tup[1], reverse=True)\n",
    "\t\t\tchild = greedy_optimizer(sorted_sentences, self._max_length)\n",
    "\n",
    "\t\t\tchildren.append(child)\n",
    "\n",
    "\t\treturn children\n",
    "\n",
    "\tdef initial_population(self):\n",
    "\t\tinitial_population = self._generate_random_population(self._population_size)\n",
    "\t\tprint (\"initial population len:\", len(initial_population))\n",
    "\t\treturn initial_population\n",
    "\n",
    "\tdef _is_better(self, scored_individual, best_scored_individual):\n",
    "\t\tif self._maximization:\n",
    "\t\t\treturn scored_individual[1] > best_scored_individual[1]\n",
    "\t\treturn scored_individual[1] < best_scored_individual[1]\n",
    "\n",
    "\tdef evolve(self, epoch):\n",
    "\t\tpopulation = self.initial_population()\n",
    "\t\tif self._maximization:\n",
    "\t\t\tbest_individual = (None, -10000)\n",
    "\t\telse:\n",
    "\t\t\tbest_individual = (None, 10000)\n",
    "\t\tfor i in range(epoch):\n",
    "\t\t\tprint (\"epoch: \", i, \" -- best individual: \", best_individual)\n",
    "\t\t\tscored_population = self._score_population(population)\n",
    "\t\t\tsorted_population = sorted(scored_population, key=lambda tup: tup[1], reverse=self._maximization)\n",
    "\t\t\tbest_individual_in_generation = sorted_population[0]\n",
    "\n",
    "\t\t\tif self._is_better(best_individual_in_generation, best_individual):\n",
    "\t\t\t\tbest_individual = best_individual_in_generation\n",
    "\n",
    "\t\t\tpopulation = self._new_generation(scored_population)\n",
    "\t\t\n",
    "\t\treturn best_individual\n",
    "\t\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "C2ThDNRwMXS3",
    "outputId": "7695c5e0-52e0-422e-99c0-8eefe39992cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic Algorithm example:\n",
      "epoch:  0  -- best individual:  (None, 10000)\n",
      "epoch:  1  -- best individual:  (['title of the third document', 'another one', 'one sentence quite random', 'que pasa'], 0.45)\n",
      "epoch:  2  -- best individual:  (['second title', 'another one', 'one sentence quite random', 'sentence here'], 0.62)\n",
      "epoch:  3  -- best individual:  (['title of the first document', 'third sentence of the first document here', 'one sentence quite random', 'sentence here'], 0.52)\n",
      "epoch:  4  -- best individual:  (['title of the first document', 'another one', 'one sentence quite random', 'sentence here'], 0.68)\n",
      "epoch:  5  -- best individual:  (['second title', 'another one', 'third sentence of the first document here', 'sentence here'], 0.70)\n",
      "epoch:  6  -- best individual:  (['title of the third document', 'another one', 'one sentence quite random', 'sentence here'], 0.73)\n",
      "epoch:  7  -- best individual:  (['second title', 'another one', 'one sentence quite random', 'sentence here'], 0.52)\n",
      "epoch:  8  -- best individual:  (['second title', 'another one', 'third sentence of the first document here', 'sentence here'], 0.78)\n",
      "epoch:  9  -- best individual:  (['second title', 'another one', 'one sentence quite random', 'sentence here'], 0.72)\n",
      "epoch:  10  -- best individual:  (['title of the third document', 'another one', 'one sentence quite random', 'que pasa'], 0.66)\n",
      "epoch:  11  -- best individual:  (['title of the third document', 'another one', 'one sentence quite random', 'sentence here'], 0.71)\n",
      "epoch:  12  -- best individual:  (['title of the first document', 'another one', 'one sentence quite random', 'sentence here'], 0.79)\n",
      "epoch:  13  -- best individual:  (['second title', 'third sentence of the first document here', 'one sentence quite random', 'sentence here'], 0.80)\n",
      "epoch:  14  -- best individual:  (['second title', 'another one', 'one sentence quite random', 'sentence here'], 0.75)\n",
      "epoch:  15  -- best individual:  (['title of the third document', 'another one', 'one sentence quite random', 'sentence here'], 0.78)\n",
      "epoch:  16  -- best individual:  (['title of the first document', 'another one', 'one sentence quite random', 'sentence here'], 0.78)\n",
      "epoch:  17  -- best individual:  (['second title', 'another one', 'one sentence quite random', 'sentence here'], 0.79)\n",
      "epoch:  18  -- best individual:  (['second title', 'another one', 'one sentence quite random', 'sentence here'], 0.74)\n",
      "epoch:  19  -- best individual:  (['second title', 'another one', 'one sentence quite random', 'sentence here'], 0.78)\n",
      "['second title', 'another one', 'one sentence quite random', 'sentence here'], 0.77)\n",
      "None\n",
      "\n",
      "==================\n",
      "\n",
      "KLDivergence: 0.26562399953846005\n",
      "JSDivergence: 0.1432920268612694\n"
     ]
    }
   ],
   "source": [
    "def JS_Gen(docs, length_max, epoch, population_size=1000):\n",
    "\tsentences = []\n",
    "\tfor title, doc in docs:\n",
    "\t\tsentences.append(title)\n",
    "\t\tsentences.extend(doc)\n",
    "\n",
    "\tdoc_freq = compute_tf(sentences)\n",
    "\t\n",
    "\tgen_optimizer = GeneticOptimizer(fitness_fun=js_divergence, \n",
    "\t\t\t\t\t\t\t\t\t docs=docs, \n",
    "\t\t\t\t\t\t\t\t\t docs_representation = doc_freq,\n",
    "\t\t\t\t\t\t\t\t\t max_length=length_max, \n",
    "\t\t\t\t\t\t\t\t\t population_size=population_size, \n",
    "\t\t\t\t\t\t\t\t\t survival_rate=0.4,\n",
    "\t\t\t\t\t\t\t\t\t mutation_rate=0.2,\n",
    "\t\t\t\t\t\t\t\t\t reproduction_rate=0.4,  \n",
    "\t\t\t\t\t\t\t\t\t maximization=False)\n",
    "\n",
    "\treturn GA_Steps()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tdoc_1 = (\"title of the first document\", [\"first sentence of first doc\", \"second sentence\", \"third sentence of the first document here\", \"another one\", \"what is going on \"])\n",
    "\tdoc_2 = (\"second title\", [\"one sentence quite random\", \"here is another one completely random\", \"sentence here\", \"que pasa\", \"a sentence in an other document\"])\n",
    "\tdoc_3 = (\"title of the third document\", [\"it will be a short docuemnt\", \"only two sentences\"])\n",
    "\tdocs = [doc_1, doc_2, doc_3]\n",
    "\n",
    "\tlength_max = 10\n",
    "\tepoch = 19\n",
    "\tpopulation_size = 10\n",
    "\tprint (\"Genetic Algorithm example:\")\n",
    "\tprint (JS_Gen(docs, length_max, epoch, population_size))\n",
    "\t#print(GA_Steps())\n",
    "\tprint (\"\\n==================\\n\")\n",
    "\tx=10\n",
    "\timport math\n",
    "\tProb_P_KL=0.32\n",
    "\tProb_Q_KL=0.18\n",
    "\tProb_A_KL=(Prob_P_KL+Prob_Q_KL)/2\n",
    "\tKLDivergence=Prob_P_KL*(math.log2(Prob_P_KL/Prob_Q_KL))\n",
    "\tKLDivergence_PA=Prob_P_KL*(math.log2(Prob_P_KL/Prob_A_KL))\n",
    "\tKLDivergence_QA=Prob_Q_KL*(math.log2(Prob_Q_KL/Prob_A_KL))\n",
    "\tprint(\"KLDivergence:\", KLDivergence)\n",
    "    \n",
    "    \n",
    "\tJSDivergence=x*(KLDivergence_PA+KLDivergence_QA)/2\n",
    "\tprint(\"JSDivergence:\", JSDivergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysZRIpGNMXS5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Swarm+Project+code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
