{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division, unicode_literals\n",
    "\n",
    "# Get the python version (used to try decode an unknow instance to unicode)\n",
    "from sys import version_info\n",
    "\n",
    "PY3 = version_info[0] == 3\n",
    "\n",
    "# Use classical Snowball stemmer for english\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# class WordTokenizer(object):\n",
    "# \t\"\"\"docstring for WordTokenizer\"\"\"\n",
    "# \tdef __init__(self):\n",
    "# \t\tpass\n",
    "# \tdef tokenize(self, arg):\n",
    "# \t\treturn nltk.word_tokenize(arg)\n",
    "# tokenizer =  WordTokenizer()\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopset = frozenset(stopwords.words('english'))\n",
    "\n",
    "# Convert an object to its unicode representation (if possible)\n",
    "def to_unicode(object):\n",
    "\tif isinstance(object, str):\n",
    "\t\treturn object\n",
    "\telif isinstance(object, bytes):\n",
    "\t\treturn object.decode(\"utf8\")\n",
    "\telse:\n",
    "\t\tprint (object)\n",
    "\t\tif PY3:\n",
    "\t\t\tif hasattr(instance, \"__str__\"):\n",
    "\t\t\t\treturn unicode(instance)\n",
    "\t\t\telif hasattr(instance, \"__bytes__\"):\n",
    "\t\t\t\treturn bytes(instance).decode(\"utf8\")\n",
    "\t\telse:\n",
    "\t\t\tif hasattr(instance, \"__unicode__\"):\n",
    "\t\t\t\treturn unicode(instance)\n",
    "\t\t\telif hasattr(instance, \"__str__\"):\n",
    "\t\t\t\treturn bytes(instance).decode(\"utf8\")\n",
    "\n",
    "# normalize and stem the word\n",
    "def stem_word(word):\n",
    "\treturn stemmer.stem(normalize_word(word))\n",
    "\n",
    "# convert to unicode and convert to lower case\n",
    "def normalize_word(word):\n",
    "\treturn to_unicode(word).lower()\n",
    "\n",
    "# convert the sentence to a list of tokens\n",
    "def sentence_tokenizer(sentence):\n",
    "\treturn tokenizer.tokenize(sentence)\n",
    "\n",
    "def get_len(element):\n",
    "\treturn len(tokenizer.tokenize(element))\n",
    "\n",
    "def get_ngrams(sentence, N):\n",
    "\ttokens = tokenizer.tokenize(sentence.lower())\n",
    "\tclean = [stemmer.stem(token) for token in tokens]\n",
    "\treturn [gram for gram in ngrams(clean, N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_utils import *\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_all_content_words_lemmatized(sentences, N=1):\n",
    "\tall_words = []\n",
    "\tfor s in sentences:\n",
    "\t\tall_words.extend([wordnet_lemmatizer.lemmatize(r) for r in tokenizer.tokenize(s)])\n",
    "\tif N == 1:\n",
    "\t\tcontent_words = [w for w in all_words if w not in stopset]\n",
    "\tnormalized_content_words = map(normalize_word, content_words)\n",
    "\tif N > 1:\n",
    "\t\treturn [gram for gram in ngrams(normalized_content_words, N)]\n",
    "\treturn normalized_content_words\n",
    "\n",
    "def get_all_content_words_stemmed(sentences, N=1):\n",
    "\tdef is_ngram_content(g):\n",
    "\t\tfor a in g:\n",
    "\t\t\tif not(a in stopset):\n",
    "\t\t\t\treturn True\n",
    "\t\treturn False\n",
    "\n",
    "\tall_words = []\n",
    "\tfor s in sentences:\n",
    "\t\tall_words.extend([stemmer.stem(r) for r in tokenizer.tokenize(s)])\n",
    "\n",
    "\tif N == 1:\n",
    "\t\tcontent_words = [w for w in all_words if w not in stopset]\n",
    "\telse:\n",
    "\t\tcontent_words = all_words\n",
    "\n",
    "\tnormalized_content_words = map(normalize_word, content_words)\n",
    "\tif N > 1:\n",
    "\t\treturn [gram for gram in ngrams(normalized_content_words, N) if is_ngram_content(gram)]\n",
    "\treturn normalized_content_words\n",
    "\n",
    "def get_all_content_words(sentences, N=1):\n",
    "\tall_words = []\n",
    "\tfor s in sentences:\n",
    "\t\tall_words.extend(tokenizer.tokenize(s))\n",
    "\tcontent_words = [w for w in all_words if w not in stopset]\n",
    "\tnormalized_content_words = map(normalize_word, content_words)\n",
    "\tif N > 1:\n",
    "\t\treturn [gram for gram in ngrams(normalized_content_words, N)]\n",
    "\treturn normalized_content_words\n",
    "\n",
    "def get_content_words_in_sentence(sentence):\n",
    "\twords = tokenizer.tokenize(sentence)\n",
    "\treturn [w for w in words if w not in stopset]\n",
    "\n",
    "def compute_tf_doc(docs, N=1):\n",
    "\tsentences = []\n",
    "\tfor title, doc in docs:\n",
    "\t\tsentences.append(title)\n",
    "\t\tsentences.extend(doc)\n",
    "\n",
    "\tcontent_words = list(set(get_all_content_words_stemmed(sentences, N)))\n",
    "\tdocs_words = []\n",
    "\tfor title, doc in docs:\n",
    "\t\ts_tmp = [title]\n",
    "\t\ts_tmp.extend(doc)\n",
    "\t\tdocs_words.append(get_all_content_words_stemmed(s_tmp, N))\n",
    "\n",
    "\tword_freq = {}\n",
    "\tfor w in content_words:\n",
    "\t\tw_score = 0\n",
    "\t\tfor d in docs_words:\n",
    "\t\t\tif w in d:\n",
    "\t\t\t\tw_score += 1\n",
    "\t\tif w_score != 0:\n",
    "\t\t\tword_freq[w] = w_score\n",
    "\n",
    "\tcontent_word_tf = dict((w, f / float(len(word_freq.keys()))) for w, f in word_freq.items())\n",
    "\treturn content_word_tf\n",
    "\n",
    "def compute_word_freq(words):\n",
    "\tword_freq = {}\n",
    "\tfor w in words:\n",
    "\t\tword_freq[w] = word_freq.get(w, 0) + 1\n",
    "\treturn word_freq\n",
    "\n",
    "def compute_tf(sentences, N=1):\n",
    "\tcontent_words = get_all_content_words_stemmed(sentences, N)\n",
    "\tcontent_words_count = len(list(content_words))\n",
    "\tcontent_words_freq = compute_word_freq(content_words)\n",
    "\n",
    "\tcontent_word_tf = dict((w, f / float(content_words_count)) for w, f in content_words_freq.items())\n",
    "\treturn content_word_tf\n",
    "\n",
    "def compute_average_freq(l_freq_1, l_freq_2):\n",
    "\taverage_freq = {}\n",
    "\n",
    "\tkeys = set(l_freq_1.keys()) | set(l_freq_2.keys())\n",
    "\n",
    "\tfor k in keys:\n",
    "\t\ts_1 = l_freq_1.get(k, 0)\n",
    "\t\ts_2 = l_freq_2.get(k, 0)\n",
    "\n",
    "\t\taverage_freq[k] = (s_1 + s_2) / 2.\n",
    "\n",
    "\treturn average_freq\n",
    "\n",
    "def kl_divergence(summary_freq, doc_freq):\n",
    "\tsum_val = 0\n",
    "\tfor w, f in summary_freq.items():\n",
    "\t\tif w in doc_freq:\n",
    "\t\t\tsum_val += f * math.log(f / float(doc_freq[w]))\n",
    "\t\n",
    "\treturn sum_val\n",
    "\n",
    "def js_divergence(sys_summary, doc_freq):\n",
    "\tsummary_freq = compute_tf(sys_summary)\n",
    "\taverage_freq = compute_average_freq(summary_freq, doc_freq)\n",
    "\n",
    "\tjsd = kl_divergence(summary_freq, average_freq) + kl_divergence(doc_freq, average_freq)\n",
    "\treturn jsd / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_utils import *\n",
    "\n",
    "def get_len(element):\n",
    "\treturn len(tokenizer.tokenize(element))\n",
    "\t\n",
    "def greedy_optimizer(sorted_elements, K):\n",
    "\tavailable_space = K\n",
    "\tsummary = []\n",
    "\n",
    "\twhile True:\n",
    "\t\tfor element in sorted_elements:\n",
    "\t\t\tlen_element = get_len(element[0])\n",
    "\t\t\tif available_space - len_element >= 0:\n",
    "\t\t\t\tidx = sorted_elements.index(element)\n",
    "\t\t\t\tdel sorted_elements[idx]\n",
    "\t\t\t\tavailable_space -= len_element\n",
    "\t\t\t\tsummary.append(element[0])\n",
    "\t\telse:\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "#from greedy import greedy_optimizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "class GeneticOptimizer(object):\n",
    "\tdef __init__(self, fitness_fun, docs, docs_representation, max_length, population_size, survival_rate, mutation_rate, reproduction_rate, maximization=False, sentences_rep=None):\n",
    "\t\tnp.random.seed(123)\n",
    "\n",
    "\t\tself._fitness_fun = fitness_fun\n",
    "\t\tself._population_size = population_size\n",
    "\t\tself._survival_rate = survival_rate\n",
    "\t\tself._mutation_rate = mutation_rate\n",
    "\t\tself._reproduction_rate = reproduction_rate\n",
    "\t\tself._maximization = maximization\n",
    "\n",
    "\t\tself._docs = docs\n",
    "\t\tself._docs_representation = docs_representation\n",
    "\t\tself._sentences_rep = sentences_rep\n",
    "\t\tself._max_length = max_length\n",
    "\t\t\n",
    "\t\tself._sentences = []\n",
    "\t\tself._sentence_tokens = []\n",
    "\t\tfor title, doc in docs:\n",
    "\t\t\tself._sentences.append(title)\n",
    "\t\t\tself._sentence_tokens.append(tokenizer.tokenize(title))\n",
    "\t\t\tself._sentences.extend(doc)\n",
    "\t\t\tfor s in doc:\n",
    "\t\t\t\tself._sentence_tokens.append(tokenizer.tokenize(s))\n",
    "\n",
    "\tdef _create_random_individual(self):\n",
    "\t\trandom_scores = np.random.rand(len(self._sentences))\n",
    "\t\tscored_sentences = zip(self._sentences, random_scores)\n",
    "\t\tsorted_sentences = sorted(scored_sentences, key=lambda tup: tup[1], reverse=True)\n",
    "\t\treturn greedy_optimizer(sorted_sentences, self._max_length)\n",
    "\n",
    "\tdef _generate_random_population(self, n):\n",
    "\t\tpopulation = []\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tpopulation.append(self._create_random_individual())\n",
    "\t\treturn population\n",
    "\n",
    "\tdef _score_population(self, population):\n",
    "\t\tscored_population = []\n",
    "\t\tfor individual in population:\n",
    "\t\t\t# score = self._fitness_fun(individual, self._docs)\n",
    "\t\t\tif self._sentences_rep != None:\n",
    "\t\t\t\tscore = self._fitness_fun(individual, self._docs_representation, self._sentences_rep)\n",
    "\t\t\telse:\n",
    "\t\t\t\tscore = self._fitness_fun(individual, self._docs_representation)\n",
    "\t\t\tscored_population.append((individual, score))\n",
    "\n",
    "\t\treturn scored_population\n",
    "\n",
    "\tdef _select_survivors(self, scored_population):\n",
    "\t\tsorted_population = sorted(scored_population, key=lambda tup: tup[1], reverse=self._maximization)\n",
    "\n",
    "\t\tpercentage_winner = 0.5\n",
    "\n",
    "\t\tto_keep = int(self._survival_rate * self._population_size)\n",
    "\t\tnumber_winners = int(percentage_winner * to_keep)\n",
    "\t\twinners = [tup[0] for tup in sorted_population[:number_winners]]\n",
    "\t\t\n",
    "\t\tlosers = sorted_population[number_winners:]\n",
    "\n",
    "\t\tnumber_losers = int((1 - percentage_winner) * to_keep) \n",
    "\n",
    "\t\tsurvivors = deepcopy(winners)\n",
    "\t\trandom_scores = np.random.rand(len(losers))\n",
    "\n",
    "\t\tsorted_losers = sorted(zip(losers, random_scores), key=lambda tup: tup[1])\n",
    "\t\tloser_survivors = [tup[0][0] for tup in sorted_losers[:number_losers]]\n",
    "\n",
    "\t\tsurvivors.extend(loser_survivors)\n",
    "\t\treturn survivors, winners\n",
    "\n",
    "\tdef _new_generation(self, scored_population):\n",
    "\t\tnew_generation, winners = self._select_survivors(scored_population)\n",
    "\t\tnew_generation = self._mutate(new_generation)\n",
    "\t\tnew_generation.extend(self._reproduction(winners, len(new_generation)))\n",
    "\t\tindividuals_to_create = self._population_size - len(new_generation)\n",
    "\t\tnew_generation.extend(self._generate_random_population(individuals_to_create))\n",
    "\t\t\n",
    "\t\treturn new_generation\n",
    "\n",
    "\tdef _len_individual(self, individual):\n",
    "\t\tlen_ = 0\n",
    "\t\tfor sentence in individual:\n",
    "\t\t\tlen_ += len(tokenizer.tokenize(sentence))\n",
    "\t\treturn len_\n",
    "\n",
    "\tdef _mutate(self, population, mutation_rate=\"auto\"):\n",
    "\t\tif mutation_rate == \"auto\":\n",
    "\t\t\tmutation_rate = self._mutation_rate\n",
    "\n",
    "\t\tnb_mutant = int(mutation_rate * len(population))\n",
    "\n",
    "\t\trandom_scores = np.random.rand(len(population))\n",
    "\t\tsorted_population = sorted(zip(population, random_scores), key=lambda tup: tup[1])\n",
    "\t\tmutants = [tup[0] for tup in sorted_population[:nb_mutant]]\n",
    "\n",
    "\t\tmutated = []\n",
    "\t\ti = 0\n",
    "\t\tfor mutant in mutants:\n",
    "\t\t\tto_mutate = deepcopy(mutant)\n",
    "\n",
    "\t\t\tsentence_to_remove = random.choice(to_mutate)\n",
    "\t\t\tidx = to_mutate.index(sentence_to_remove)\n",
    "\t\t\tdel to_mutate[idx]\n",
    "\n",
    "\t\t\tavailable_size = self._max_length - self._len_individual(to_mutate)\n",
    "\t\t\n",
    "\t\t\tavailable_sentences = [s[0] for s in zip(self._sentences, self._sentence_tokens) if len(s[1]) <= available_size]\n",
    "\t\t\tif available_sentences != []:\n",
    "\t\t\t\ti += 1\n",
    "\t\t\t\tsentence_to_add = random.choice(available_sentences)\n",
    "\t\t\t\tto_mutate.append(sentence_to_add)\n",
    "\t\t\t\t\n",
    "\t\t\t\tmutated.append(to_mutate)\n",
    "\t\t\n",
    "\t\tpopulation.extend(mutated)\n",
    "\t\treturn population\n",
    "\n",
    "\tdef _reproduction(self, population_winners, population_size, reproduction_rate=\"auto\"):\n",
    "\t\tif reproduction_rate == \"auto\":\n",
    "\t\t\treproduction_rate = self._reproduction_rate\n",
    "\n",
    "\t\tparents = []\n",
    "\t\tnumber_families = int(reproduction_rate * population_size)\n",
    "\t\n",
    "\t\tfor i in range(number_families):\n",
    "\t\t\tparents.append(random.sample(population_winners, 2))\n",
    "\n",
    "\t\tchildren = []\n",
    "\t\tfor father, mother in parents:\n",
    "\t\t\tgenetic_pool = [s for s in self._sentences if s in father]\n",
    "\t\t\tgenetic_pool.extend([s for s in self._sentences if s in mother])\n",
    "\t\t\trandom_scores = np.random.rand(len(genetic_pool))\n",
    "\t\t\tscored_sentences = zip(self._sentences, random_scores)\n",
    "\t\t\tsorted_sentences = sorted(scored_sentences, key=lambda tup: tup[1], reverse=True)\n",
    "\t\t\tchild = greedy_optimizer(sorted_sentences, self._max_length)\n",
    "\n",
    "\t\t\tchildren.append(child)\n",
    "\n",
    "\t\treturn children\n",
    "\n",
    "\tdef initial_population(self):\n",
    "\t\tinitial_population = self._generate_random_population(self._population_size)\n",
    "\t\tprint (\"initial population len:\", len(initial_population))\n",
    "\t\treturn initial_population\n",
    "\n",
    "\tdef _is_better(self, scored_individual, best_scored_individual):\n",
    "\t\tif self._maximization:\n",
    "\t\t\treturn scored_individual[1] > best_scored_individual[1]\n",
    "\t\treturn scored_individual[1] < best_scored_individual[1]\n",
    "\n",
    "\tdef evolve(self, epoch):\n",
    "\t\tpopulation = self.initial_population()\n",
    "\t\tif self._maximization:\n",
    "\t\t\tbest_individual = (None, -10000)\n",
    "\t\telse:\n",
    "\t\t\tbest_individual = (None, 10000)\n",
    "\t\tfor i in range(epoch):\n",
    "\t\t\tprint (\"epoch: \", i, \" -- best individual: \", best_individual)\n",
    "\t\t\tscored_population = self._score_population(population)\n",
    "\t\t\tsorted_population = sorted(scored_population, key=lambda tup: tup[1], reverse=self._maximization)\n",
    "\t\t\tbest_individual_in_generation = sorted_population[0]\n",
    "\n",
    "\t\t\tif self._is_better(best_individual_in_generation, best_individual):\n",
    "\t\t\t\tbest_individual = best_individual_in_generation\n",
    "\n",
    "\t\t\tpopulation = self._new_generation(scored_population)\n",
    "\t\t\n",
    "\t\treturn best_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "#from greedy import greedy_optimizer\n",
    "\n",
    "#from JS import js_divergence\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "class SwarmOptimizer(object):\n",
    "\tdef __init__(self, fitness_fun, docs,  docs_representation, max_length, number_locations, trial_limit, mfe, args=None, maximization=False):\n",
    "\t\tnp.random.seed(123)\n",
    "\n",
    "\t\tself._fitness_fun = fitness_fun\n",
    "\t\tself._args = args\n",
    "\t\tself._maximization = maximization\n",
    "\n",
    "\t\tself._number_locations = number_locations\n",
    "\t\tself._trial_limit = trial_limit\n",
    "\t\tself._mfe = mfe\n",
    "\n",
    "\t\tself._docs = docs\n",
    "\t\tself._docs_representation = docs_representation\n",
    "\t\tself._max_length = max_length\n",
    "\n",
    "\t\tself._sentences = []\n",
    "\t\tself._sentence_tokens = []\n",
    "\t\tfor title, doc in docs:\n",
    "\t\t\tself._sentences.append(title)\n",
    "\t\t\tself._sentence_tokens.append(tokenizer.tokenize(title))\n",
    "\t\t\tself._sentences.extend(doc)\n",
    "\t\t\tfor s in doc:\n",
    "\t\t\t\tself._sentence_tokens.append(tokenizer.tokenize(s))\n",
    "\n",
    "\n",
    "\tdef _create_random_food_location(self):\n",
    "\t\trandom_scores = np.random.rand(len(self._sentences))\n",
    "\t\tscored_sentences = zip(self._sentences, random_scores)\n",
    "\t\tsorted_sentences = sorted(scored_sentences, key=lambda tup: tup[1], reverse=True)\n",
    "\t\treturn greedy_optimizer(sorted_sentences, self._max_length)\n",
    "\n",
    "\tdef _generate_random_foods(self, n):\n",
    "\t\tfoods = []\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tfoods.append(self._create_random_food_location())\n",
    "\t\treturn foods\n",
    "\n",
    "\tdef _len_location(self, location):\n",
    "\t\tlen_ = 0\n",
    "\t\tfor sentence in location:\n",
    "\t\t\tlen_ += len(tokenizer.tokenize(sentence))\n",
    "\t\treturn len_\n",
    "\n",
    "\tdef _score_food_location(self, food_location):\n",
    "\t\tif self._args:\n",
    "\t\t\treturn self._fitness_fun(food_location, self._docs_representation, self._args)\n",
    "\t\treturn self._fitness_fun(food_location, self._docs_representation)\n",
    "\n",
    "\tdef _random_local_search(self, food_location_old):\n",
    "\t\tfood_location = deepcopy(food_location_old)\n",
    "\t\tsentence_to_remove = random.choice(food_location)\n",
    "\t\tidx = food_location.index(sentence_to_remove)\n",
    "\t\tdel food_location[idx]\n",
    "\n",
    "\t\tavailable_size = self._max_length - self._len_location(food_location)\n",
    "\t\t\n",
    "\t\tavailable_sentences = [s[0] for s in zip(self._sentences, self._sentence_tokens) if len(s[1]) <= available_size]\n",
    "\t\tif available_sentences != []:\n",
    "\t\t\tsentence_to_add = random.choice(available_sentences)\n",
    "\t\t\tfood_location.append(sentence_to_add)\n",
    "\t\t\t\t\n",
    "\t\treturn food_location\n",
    "\n",
    "\tdef initial_foods(self):\n",
    "\t\tinitial_foods = self._generate_random_foods(self._number_locations)\n",
    "\t\tprint (\"initial number of foods :\", len(initial_foods))\n",
    "\t\treturn initial_foods\n",
    "\n",
    "\tdef _is_better(self, score_a, score_b):\n",
    "\t\tif self._maximization:\n",
    "\t\t\treturn score_a > score_b\n",
    "\t\treturn score_a < score_b\n",
    "\n",
    "\tdef swarm_disperse(self):\n",
    "\t\tfoods_locations = self.initial_foods()\n",
    "\t\tscore_vector = [self._score_food_location(loc) for loc in foods_locations]\n",
    "\t\ttrials = [0] * len(foods_locations)\n",
    "\t\tnumber_fitness_evalutation = len(foods_locations)\n",
    "\n",
    "\t\tsize_10_percent = int(0.1 * len(foods_locations))\n",
    "\t\tif self._maximization:\n",
    "\t\t\tbest_location = (None, -10000)\n",
    "\t\telse:\n",
    "\t\t\tbest_location = (None, 10000)\n",
    "\n",
    "\t\tepoch = 0\n",
    "\t\twhile True:\n",
    "\t\t\tepoch += 1\n",
    "\t\t\tprint (\"epoch: \", epoch, \" -- best location: \", best_location)\n",
    "\n",
    "\t\t\t# Employed Bees Phase\n",
    "\t\t\tfor i, location in enumerate(foods_locations):\n",
    "\t\t\t\tnew_location = self._random_local_search(location)\n",
    "\t\t\t\tscore = self._score_food_location(new_location)\n",
    "\t\t\t\tnumber_fitness_evalutation += 1\n",
    "\n",
    "\t\t\t\tif self._is_better(score, score_vector[i]):\n",
    "\t\t\t\t\tif self._is_better(score, best_location[1]):\n",
    "\t\t\t\t\t\tbest_location = (new_location, score)\n",
    "\n",
    "\t\t\t\t\tscore_vector[i] = score\n",
    "\t\t\t\t\tfoods_locations[i] = new_location\n",
    "\t\t\t\t\ttrials[i] = 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttrials[i] += 1\n",
    "\n",
    "\t\t\t\tif number_fitness_evalutation >= self._mfe:\n",
    "\t\t\t\t\treturn best_location\n",
    "\n",
    "\t\t\tsum_ = sum(score_vector)\n",
    "            \n",
    "\t\t\tif sum_ == 0:\n",
    "\t\t\t\tprobability_vector = [100 for s in score_vector]\n",
    "\t\t\telse:\n",
    "\t\t\t\tprobability_vector = [float(s) / float(sum_) for s in score_vector]\n",
    "\n",
    "\t\t\t# Onlooker Bees Phase\n",
    "\t\t\ts = 0\n",
    "\t\t\tt = 1\n",
    "\t\t\twhile t < self._number_locations:\n",
    "\t\t\t\tr = random.uniform(0, 1)\n",
    "\t\t\t\tif r < probability_vector[s]:\n",
    "\t\t\t\t\tt += 1\n",
    "\t\t\t\t\tlocation = foods_locations[s]\n",
    "\t\t\t\t\tnew_location = self._random_local_search(location)\n",
    "\t\t\t\t\tscore = self._score_food_location(new_location)\n",
    "\t\t\t\t\tnumber_fitness_evalutation += 1\n",
    "\n",
    "\t\t\t\t\tif self._is_better(score, score_vector[i]):\n",
    "\t\t\t\t\t\tif self._is_better(score, best_location[1]):\n",
    "\t\t\t\t\t\t\tbest_location = (new_location, score)\n",
    "\n",
    "\t\t\t\t\t\tscore_vector[i] = score\n",
    "\t\t\t\t\t\tfoods_locations[i] = new_location\n",
    "\t\t\t\t\t\ttrials[i] = 0\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttrials[i] += 1\n",
    "\n",
    "\t\t\t\t\tif number_fitness_evalutation >= self._mfe:\n",
    "\t\t\t\t\t\treturn best_location\n",
    "\t\t\t\ts += 1\n",
    "\t\t\t\tif s == self._number_locations:\n",
    "\t\t\t\t\ts = 0\n",
    "\n",
    "\t\t\t# Scout Bees Phase\n",
    "\t\t\tmi = trials.index(max(trials))\n",
    "\t\t\tif trials[mi] > self._trial_limit:\n",
    "\t\t\t\tnew_location = self._create_random_food_location()\n",
    "\t\t\t\tscore = self._score_food_location(new_location)\n",
    "\t\t\t\tnumber_fitness_evalutation += 1\n",
    "\t\t\t\ttrials[mi] = 0\n",
    "\n",
    "\t\t\t\tif self._is_better(score, best_location[1]):\n",
    "\t\t\t\t\tbest_location = (new_location, score)\n",
    "\n",
    "\t\t\t\tif number_fitness_evalutation >= self._mfe:\n",
    "\t\t\t\t\treturn best_location\n",
    "\n",
    "\t\treturn best_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic Algorithm example:\n",
      "initial population len: 10\n",
      "epoch:  0  -- best individual:  (None, 10000)\n",
      "epoch:  1  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  2  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  3  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  4  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  5  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  6  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  7  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  8  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  9  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  10  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  11  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  12  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  13  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  14  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  15  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  16  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  17  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  18  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "epoch:  19  -- best individual:  (['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "(['second title', 'another one', 'one sentnece quite random', 'sentence here'], 0.0)\n",
      "\n",
      "==================\n",
      "\n",
      "Swarm Intelligence example:\n",
      "initial number of foods : 10\n",
      "epoch:  1  -- best location:  (None, 10000)\n",
      "epoch:  2  -- best location:  (None, 10000)\n",
      "epoch:  3  -- best location:  (None, 10000)\n",
      "epoch:  4  -- best location:  (None, 10000)\n",
      "epoch:  5  -- best location:  (None, 10000)\n",
      "epoch:  6  -- best location:  (None, 10000)\n",
      "epoch:  7  -- best location:  (None, 10000)\n",
      "epoch:  8  -- best location:  (None, 10000)\n",
      "epoch:  9  -- best location:  (None, 10000)\n",
      "epoch:  10  -- best location:  (None, 10000)\n",
      "epoch:  11  -- best location:  (None, 10000)\n",
      "epoch:  12  -- best location:  (None, 10000)\n",
      "epoch:  13  -- best location:  (None, 10000)\n",
      "epoch:  14  -- best location:  (None, 10000)\n",
      "epoch:  15  -- best location:  (None, 10000)\n",
      "epoch:  16  -- best location:  (None, 10000)\n",
      "epoch:  17  -- best location:  (None, 10000)\n",
      "epoch:  18  -- best location:  (None, 10000)\n",
      "epoch:  19  -- best location:  (None, 10000)\n",
      "epoch:  20  -- best location:  (None, 10000)\n",
      "epoch:  21  -- best location:  (None, 10000)\n",
      "(None, 10000)\n"
     ]
    }
   ],
   "source": [
    "def JS_Gen(docs, length_max, epoch, population_size=1000):\n",
    "\tsentences = []\n",
    "\tfor title, doc in docs:\n",
    "\t\tsentences.append(title)\n",
    "\t\tsentences.extend(doc)\n",
    "\n",
    "\tdoc_freq = compute_tf(sentences)\n",
    "\t\n",
    "\tgen_optimizer = GeneticOptimizer(fitness_fun=js_divergence, \n",
    "\t\t\t\t\t\t\t\t\t docs=docs, \n",
    "\t\t\t\t\t\t\t\t\t docs_representation = doc_freq,\n",
    "\t\t\t\t\t\t\t\t\t max_length=length_max, \n",
    "\t\t\t\t\t\t\t\t\t population_size=population_size, \n",
    "\t\t\t\t\t\t\t\t\t survival_rate=0.4,\n",
    "\t\t\t\t\t\t\t\t\t mutation_rate=0.2,\n",
    "\t\t\t\t\t\t\t\t\t reproduction_rate=0.4,  \n",
    "\t\t\t\t\t\t\t\t\t maximization=False)\n",
    "\n",
    "\treturn gen_optimizer.evolve(epoch)\n",
    "\n",
    "def JS_Swarm(docs, length_max, mfe=80000, number_locations=1000):\n",
    "\tsentences = []\n",
    "\tfor title, doc in docs:\n",
    "\t\tsentences.append(title)\n",
    "\t\tsentences.extend(doc)\n",
    "\n",
    "\tdoc_freq = compute_tf(sentences)\n",
    "\t\n",
    "\tswarm_optimizer = SwarmOptimizer(fitness_fun=js_divergence, \n",
    "\t\t\t\t\t\t\t\t\t docs=docs, \n",
    "\t\t\t\t\t\t\t\t\t docs_representation = doc_freq,\n",
    "\t\t\t\t\t\t\t\t\t max_length=length_max, \n",
    "\t\t\t\t\t\t\t\t\t number_locations=number_locations, \n",
    "\t\t\t\t\t\t\t\t\t trial_limit=400,\n",
    "\t\t\t\t\t\t\t\t\t mfe=mfe,  \n",
    "\t\t\t\t\t\t\t\t\t maximization=False)\n",
    "\n",
    "\treturn swarm_optimizer.swarm_disperse()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tdoc_1 = (\"title of the first document\", [\"first sentence of first doc\", \"second sentence\", \"third sentence of the first document here\", \"another one\", \"what is going on \"])\n",
    "\tdoc_2 = (\"second title\", [\"one sentnece quite random\", \"here is another one completely random\", \"sentence here\", \"que pasa\", \"a sentence in an other document\"])\n",
    "\tdoc_3 = (\"title of the third document\", [\"it will be a short docuemnt\", \"only two sentences\"])\n",
    "\tdocs = [doc_1, doc_2, doc_3]\n",
    "\n",
    "\tlength_max = 10\n",
    "\tepoch = 20\n",
    "\tpopulation_size = 10\n",
    "\tprint (\"Genetic Algorithm example:\")\n",
    "\tprint (JS_Gen(docs, length_max, epoch, population_size))\n",
    "\n",
    "\tprint (\"\\n==================\\n\")\n",
    "\tmfe = 400\n",
    "\tnumber_locations = population_size\n",
    "\tprint (\"Swarm Intelligence example:\")\n",
    "\tprint (JS_Swarm(docs, length_max, mfe, number_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
